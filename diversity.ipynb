{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a46f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sj0161/.conda/envs/py38torch113/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sj0161/.conda/envs/py38torch113/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sj0161/.conda/envs/py38torch113/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sj0161/.conda/envs/py38torch113/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sj0161/.conda/envs/py38torch113/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-11-16 12:39:54.910654: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-16 12:39:55.015032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-16 12:39:58.800579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import pickle\n",
    "import platform\n",
    "import copy\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as pplt\n",
    "import umap\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from vendi_score import vendi\n",
    "\n",
    "import sklearn.manifold as skma\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.decomposition as skd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from spektral.layers import GINConvBatch, GlobalAttentionPool, GlobalMaxPool, GlobalAttnSumPool\n",
    "\n",
    "# Import local modules\n",
    "from topo_sim.model import KLDivergenceLayer, Sampling\n",
    "\n",
    "# Configuration for file paths\n",
    "DATA_DIR = '/home/sj0161/complex_polymer/complex_polymer/temp/' # TODO: change this\n",
    "PLOT_DIR = '../fig/'\n",
    "WEIGHT_DIR = '/scratch/gpfs/sj0161/20230829/'\n",
    "\n",
    "# Set plot configurations\n",
    "pplt.rc['figure.facecolor'] = 'white'\n",
    "\n",
    "# Initialize color cycle\n",
    "COLORS = []\n",
    "colors1 = pplt.Cycle('default')\n",
    "colors2 = pplt.Cycle('538')\n",
    "\n",
    "for color in colors1:\n",
    "    COLORS.append(color['color'])\n",
    "\n",
    "for color in colors2:\n",
    "    COLORS.append(color['color'])\n",
    "\n",
    "# Handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Some constants\n",
    "LATENT_DIM = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd65bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, fold, n_fold=5, if_validation=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory path where the data is stored.\n",
    "        fold (int): Index of the fold to be used as the test set.\n",
    "        n_fold (int, optional): Number of folds to split the data into. Default is 5.\n",
    "        if_validation (bool, optional): Whether to include a validation set. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing training, validation (optional), and test datasets,\n",
    "               along with topo_class names, scaler, and label encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(data_dir, 'rb') as handle:\n",
    "        x, y, topo_desc, topo_class, poly_param, graph = [pickle.load(handle) for _ in range(6)]\n",
    "    \n",
    "    # x: graph feature\n",
    "    # y: rg2 value\n",
    "    # topo_desc: topological descriptors\n",
    "    # topo_class: topology classes\n",
    "    # poly_param: polymer generation parameters\n",
    "    # graph: networkx objects\n",
    "    \n",
    "    # preprocessing\n",
    "    y = y[..., 0]\n",
    "    \n",
    "    SCALER = StandardScaler()\n",
    "    topo_desc = SCALER.fit_transform(topo_desc)\n",
    "\n",
    "    topo_class[topo_class == 'astar'] = 'star'\n",
    "    topo_desc = np.where(np.isnan(topo_desc), -2, topo_desc) # only node assortativity has 0, should be [-1, 1]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    topo_class = le.fit_transform(topo_class)\n",
    "    NAMES = le.classes_\n",
    "    \n",
    "    # random shuffle\n",
    "    x = np.random.RandomState(0).permutation(x)\n",
    "    y = np.random.RandomState(0).permutation(y)\n",
    "    topo_class = np.random.RandomState(0).permutation(topo_class)\n",
    "    topo_desc = np.random.RandomState(0).permutation(topo_desc)\n",
    "    poly_param = np.random.RandomState(0).permutation(poly_param)\n",
    "    graph = np.random.RandomState(0).permutation(graph)\n",
    "\n",
    "    # we just use one fold for testing\n",
    "    skf = StratifiedKFold(n_splits=n_fold)\n",
    "    count = -1\n",
    "    for _, (train_idx, test_idx) in enumerate(skf.split(x, topo_class)):\n",
    "        train_data = [data[train_idx] for data in [x, y, topo_desc, topo_class, graph]]\n",
    "        test_data = [data[test_idx] for data in [x, y, topo_desc, topo_class, graph]]\n",
    "        x_train, y_train, l_train, c_train, graph_train = train_data\n",
    "        x_test, y_test, l_test, c_test, graph_test = test_data\n",
    "\n",
    "        if if_validation:\n",
    "            skf2 = StratifiedKFold(n_splits=n_fold)\n",
    "            train_idx2, valid_idx = next(iter(skf2.split(x_train, c_train)))\n",
    "            x_valid, y_valid, l_valid, c_valid, graph_valid = (\n",
    "                [data[valid_idx] for data in [x_train, y_train, l_train, c_train, graph_train]])\n",
    "            x_train, y_train, l_train, c_train, graph_train = (\n",
    "                [data[train_idx2] for data in [x_train, y_train, l_train, c_train, graph_train]])\n",
    "\n",
    "                \n",
    "        count += 1\n",
    "        if count == fold:\n",
    "            break\n",
    "\n",
    "    if if_validation:\n",
    "        print(f'Train: {len(x_train)} Valid: {len(x_valid)} Test: {len(x_test)}')\n",
    "        return ((x_train, y_train, c_train, l_train, graph_train),\n",
    "                (x_valid, y_valid, c_valid, l_valid, graph_valid),\n",
    "                (x_test, y_test, c_test, l_test, graph_test),\n",
    "                NAMES, SCALER, le)\n",
    "            \n",
    "    else:\n",
    "        print(f'Train: {len(x_train)} Test: {len(x_test)}')\n",
    "        return ((x_train, y_train, c_train, l_train, graph_train),\n",
    "                (x_test, y_test, c_test, l_test, graph_test),\n",
    "                NAMES, SCALER, le)\n",
    "\n",
    "    \n",
    "def graph_to_lap_spec(graphs):\n",
    "    lap_spec_data = []\n",
    "    for G in graphs:\n",
    "        lap_spec = nx.laplacian_spectrum(G)\n",
    "        lap_spec_zero_pad = np.zeros((100,))\n",
    "        lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "        lap_spec_data.append(lap_spec_zero_pad)\n",
    "    return np.array(lap_spec_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ee376",
   "metadata": {},
   "source": [
    "### Vendi score evaluation for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd23fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 858 Valid: 215 Test: 269\n"
     ]
    }
   ],
   "source": [
    "((x_train, y_train, c_train, l_train, graph_train),\n",
    "(x_valid, y_valid, c_valid, l_valid, graph_valid),\n",
    "(x_test, y_test, c_test, l_test, graph_test),\n",
    "NAMES, SCALER, LE) = load_data(os.path.join(DATA_DIR, 'rg2.pickle'), fold=0, if_validation=True)\n",
    "\n",
    "graph_all = np.concatenate((graph_train, graph_valid, graph_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703e634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all graphs into graph eigen spectra\n",
    "graph_total = [graph_train, graph_valid, graph_test]\n",
    "\n",
    "lap_spec_data = []\n",
    "\n",
    "for graphs in graph_total:\n",
    "    for G in graphs:\n",
    "        lap_spec = nx.laplacian_spectrum(G)\n",
    "        lap_spec_zero_pad = np.zeros((100,))\n",
    "        lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "        lap_spec_data.append(lap_spec_zero_pad)\n",
    "        \n",
    "lap_spec_data = np.array(lap_spec_data)\n",
    "\n",
    "with open(\"../result/lap_spec_data.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(lap_spec_data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d0666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../result/lap_spec_data.pickle\", \"rb\") as handle:\n",
    "    lap_spec_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c459e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 2.0968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff6b38",
   "metadata": {},
   "source": [
    "### Vendi score evaluation for the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2c9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../result/latent_space_desc_gnn_cnn.pickle\n",
      "Dataset Vendi Score: 7.3225 \n",
      "\n",
      "../result/latent_space_gnn_cnn.pickle\n",
      "Dataset Vendi Score: 7.4370 \n",
      "\n",
      "../result/latent_space_desc_dnn_cnn.pickle\n",
      "Dataset Vendi Score: 7.0863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"../result/latent_space_desc_gnn_cnn.pickle\",\n",
    "    \"../result/latent_space_gnn_cnn.pickle\",\n",
    "    \"../result/latent_space_desc_dnn_cnn.pickle\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"rb\") as handle:\n",
    "        latent_data = pickle.load(handle)\n",
    "    print(file)\n",
    "    print(f\"Dataset Vendi Score: {vendi.score_dual(latent_data):0.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a1dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../result/latent_space_False_False.pickle\n",
      "Dataset Vendi Score: 5.8532 \n",
      "\n",
      "../result/latent_space_False_True.pickle\n",
      "Dataset Vendi Score: 6.3171 \n",
      "\n",
      "../result/latent_space_True_False.pickle\n",
      "Dataset Vendi Score: 5.3128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"../result/latent_space_False_False.pickle\",\n",
    "    \"../result/latent_space_False_True.pickle\",\n",
    "    \"../result/latent_space_True_False.pickle\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"rb\") as handle:\n",
    "        latent_data = pickle.load(handle)\n",
    "    print(file)\n",
    "    print(f\"Dataset Vendi Score: {vendi.score_dual(latent_data):0.4f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62f7bf",
   "metadata": {},
   "source": [
    "### Vendi score evaluation for the random generation based on different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40133e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 5.0684\n"
     ]
    }
   ],
   "source": [
    "with open(\"../result/no_valid_random_gen_desc_gnn_cnn.pickle\", \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = graph_to_lap_spec(gen_clean_graph)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10352c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 4.9580\n"
     ]
    }
   ],
   "source": [
    "with open(\"../result/no_valid_random_gen_gnn_cnn.pickle\", \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = graph_to_lap_spec(gen_clean_graph)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4312a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 4.3305\n"
     ]
    }
   ],
   "source": [
    "with open(\"../result/no_valid_random_gen_desc_dnn_cnn.pickle\", \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = graph_to_lap_spec(gen_clean_graph)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d89c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../result/no_valid_random_gen_desc_gnn_cnn.pickle\", \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = graph_to_lap_spec(gen_clean_graph)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb22a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38torch113 [~/.conda/envs/py38torch113/]",
   "language": "python",
   "name": "conda_py38torch113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
