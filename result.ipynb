{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0da5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import platform\n",
    "import copy\n",
    "import umap\n",
    "\n",
    "import proplot as pplt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from vendi_score import vendi\n",
    "from graph_utils import *\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "from analysis_utils import *\n",
    "from saliency_utils import *\n",
    "from generation_utils import *\n",
    "\n",
    "DATA_DIR = '/scratch/gpfs/sj0161/topo_data/'\n",
    "WEIGHT_DIR = '/scratch/gpfs/sj0161/topo_result/'\n",
    "ANALYSIS_DIR = '/scratch/gpfs/sj0161/topo_analysis/'\n",
    "\n",
    "\n",
    "pplt.rc['figure.facecolor'] = 'white'\n",
    "\n",
    "COLORS = []\n",
    "colors1 = pplt.Cycle('default')\n",
    "colors2 = pplt.Cycle('538')\n",
    "\n",
    "for color in colors1:\n",
    "    COLORS.append(color['color'])\n",
    "\n",
    "for color in colors2:\n",
    "    COLORS.append(color['color'])\n",
    "\n",
    "LATENT_DIM = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d4303",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29724377",
   "metadata": {},
   "outputs": [],
   "source": [
    "((x_train, y_train, c_train, l_train, graph_train),\n",
    "(x_valid, y_valid, c_valid, l_valid, graph_valid),\n",
    "(x_test, y_test, c_test, l_test, graph_test),\n",
    "NAMES, SCALER, LE) = load_data(os.path.join(DATA_DIR, 'rg2.pickle'), fold=0, if_validation=True)\n",
    "\n",
    "graph_all = np.concatenate((graph_train, graph_valid, graph_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d93500",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection Based on Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445d4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 858 Valid: 215 Test: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [07:42<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 858 Valid: 215 Test: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [07:22<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 858 Valid: 215 Test: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [04:48<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of encoders to iterate through\n",
    "encoders = [\"desc_gnn\", \"gnn\", \"desc_dnn\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for encoder in encoders:\n",
    "    elbos, baccs, kls, cls, rls, rec, rmses, r2s, f1s, files_select = get_val_metrics(encoder=encoder)\n",
    "    \n",
    "    results.append([elbos, baccs, kls, cls, rls, rec, rmses, r2s, f1s, files_select])\n",
    "    \n",
    "with open(os.path.join(ANALYSIS_DIR, \"hyper_select_result_all.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(results, handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e1503f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ANALYSIS_DIR, \"hyper_select_result_all.pickle\"), \"rb\") as handle:\n",
    "    results = pickle.load(handle)\n",
    "\n",
    "# Concatenate arrays from all results\n",
    "baccs = np.concatenate([result[1] for result in results])\n",
    "r2s = np.concatenate([result[7] for result in results])\n",
    "f1s = np.concatenate([result[8] for result in results])\n",
    "kls = np.concatenate([result[2] for result in results])\n",
    "rmses = np.concatenate([result[6] for result in results])\n",
    "files_select = np.concatenate([result[9] for result in results])\n",
    "\n",
    "# Define indices for separating results of different encoders\n",
    "idx = [0, \n",
    "       len(results[0][0]), \n",
    "       len(results[0][0]) + len(results[0][0]), \n",
    "       len(results[0][0]) + len(results[1][0]) + len(results[2][0])]\n",
    "\n",
    "# Calculate the Pareto front and find the best points\n",
    "pareto_indices, pareto_front = pareto_frontier(1 - baccs, 1 - r2s, 1 - f1s, kls, limits=[0.1, 0.1, 0.1, np.inf])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae53cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=[\n",
    "    \"Encoder\", \"File Name\", \"BACC\", \"RMSE\", \"R2\", \"F1\", \"KL\", \"Distance to Origin\"\n",
    "])\n",
    "\n",
    "for i, encoder in enumerate(encoders):\n",
    "    \n",
    "    idx_temp = np.where((np.array(pareto_indices) >= idx[i]) & (np.array(pareto_indices) < idx[i+1]))[0]\n",
    "    \n",
    "    pareto_front_temp = pareto_front[idx_temp]\n",
    "    best_point, best_idx = closest_to_origin(pareto_front_temp)#, vmin, vmax)\n",
    "\n",
    "    # Extract information for the best point\n",
    "    encoder_name = encoder\n",
    "    file_name = files_select[best_idx].split('/')[-1].split('.pickle')[0]\n",
    "    bacc = baccs[best_idx]\n",
    "    rmse = rmses[best_idx]\n",
    "    r2 = r2s[best_idx]\n",
    "    f1 = f1s[best_idx]\n",
    "    kl = kls[best_idx]\n",
    "    distance_to_origin = np.linalg.norm(np.array(best_point[:]))\n",
    "\n",
    "    # Create a dictionary with the data for the current encoder\n",
    "    row_data = {\n",
    "        \"Encoder\": encoder_name,\n",
    "        \"File Name\": file_name,\n",
    "        \"BACC\": bacc,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"F1\": f1,\n",
    "        \"KL\": kl,\n",
    "        \"Distance to Origin\": distance_to_origin\n",
    "    }\n",
    "\n",
    "    # Append the data to the DataFrame\n",
    "    data = data.append(row_data, ignore_index=True)\n",
    "\n",
    "data.to_csv(os.path.join(ANALYSIS_DIR, \"hyper_select_result.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c9fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>File Name</th>\n",
       "      <th>BACC</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>F1</th>\n",
       "      <th>KL</th>\n",
       "      <th>Distance to Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desc_gnn</td>\n",
       "      <td>desc_gnn_cnn_20230828_8_val_decoder_acc_True_T...</td>\n",
       "      <td>0.943852</td>\n",
       "      <td>1.467548</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.995348</td>\n",
       "      <td>18.724371</td>\n",
       "      <td>0.382945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gnn</td>\n",
       "      <td>gnn_cnn_20230828_8_val_decoder_acc_True_True_1...</td>\n",
       "      <td>0.944785</td>\n",
       "      <td>3.044410</td>\n",
       "      <td>0.963435</td>\n",
       "      <td>0.976836</td>\n",
       "      <td>15.601805</td>\n",
       "      <td>0.842681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desc_dnn</td>\n",
       "      <td>desc_dnn_cnn_20230828_8_val_decoder_acc_True_T...</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>1.140703</td>\n",
       "      <td>0.994867</td>\n",
       "      <td>0.995348</td>\n",
       "      <td>16.041761</td>\n",
       "      <td>0.399231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Encoder                                          File Name      BACC  \\\n",
       "0  desc_gnn  desc_gnn_cnn_20230828_8_val_decoder_acc_True_T...  0.943852   \n",
       "1       gnn  gnn_cnn_20230828_8_val_decoder_acc_True_True_1...  0.944785   \n",
       "2  desc_dnn  desc_dnn_cnn_20230828_8_val_decoder_acc_True_T...  0.928105   \n",
       "\n",
       "       RMSE        R2        F1         KL  Distance to Origin  \n",
       "0  1.467548  0.991504  0.995348  18.724371            0.382945  \n",
       "1  3.044410  0.963435  0.976836  15.601805            0.842681  \n",
       "2  1.140703  0.994867  0.995348  16.041761            0.399231  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db1e0c",
   "metadata": {},
   "source": [
    "# Results of the Best Model of Each Encoder Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57e06c",
   "metadata": {},
   "source": [
    "### With 10 Repetitions Considering the Randomness of Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6737529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: Train 1.19+/-0.03 Valid 1.47+/-0.05 Test 1.49+/-0.06\n",
      "R2:   Train 0.99+/-0.00 Valid 0.99+/-0.00 Test 0.99+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 0.98+/-0.01 Test 0.96+/-0.01\n",
      "BACC: Train 1.00+/-0.00 Valid 0.94+/-0.00 Test 0.94+/-0.00\n",
      "RMSE: Train 2.38+/-0.05 Valid 3.16+/-0.06 Test 3.14+/-0.07\n",
      "R2:   Train 0.98+/-0.00 Valid 0.96+/-0.00 Test 0.96+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 0.97+/-0.00 Test 0.98+/-0.00\n",
      "BACC: Train 0.99+/-0.00 Valid 0.94+/-0.00 Test 0.94+/-0.00\n",
      "RMSE: Train 1.05+/-0.03 Valid 1.27+/-0.10 Test 1.99+/-0.06\n",
      "R2:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.98+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 1.00+/-0.00 Test 0.97+/-0.00\n",
      "BACC: Train 0.97+/-0.00 Valid 0.92+/-0.00 Test 0.92+/-0.00\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\",\n",
    "    \"gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 100]_0.01_64\",\n",
    "    \"desc_dnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 0.01]_0.01_64\",\n",
    "]\n",
    "\n",
    "baccs = []\n",
    "rmses = []\n",
    "r2s = []\n",
    "f1s = []\n",
    "accs = []\n",
    "train_outs = []\n",
    "valid_outs = []\n",
    "test_outs = []\n",
    "\n",
    "for file in files:\n",
    "    ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "    model, pickle_file = train_vae(ENCODER, DECODER, MONITOR,\n",
    "                  IF_REG, IF_CLS,\n",
    "                  x_train, x_valid,\n",
    "                  y_train, y_valid,\n",
    "                  c_train, c_valid,\n",
    "                  l_train, l_valid,\n",
    "                  1.0, weights,\n",
    "                  LR, BS,\n",
    "                  False, ) \n",
    "\n",
    "    train_out, valid_out, test_out, bacc, rmse, r2, f1, acc = get_metrics(model, ENCODER, IF_REG, IF_CLS,\n",
    "                                                               x_train, y_train, c_train, l_train,\n",
    "                                                               x_valid, y_valid, c_valid, l_valid,\n",
    "                                                               x_test, y_test, c_test, l_test,\n",
    "                                                               n_repeat=10, if_bacc=True)\n",
    "    baccs.append(bacc)\n",
    "    rmses.append(rmse)\n",
    "    r2s.append(r2)\n",
    "    f1s.append(f1)\n",
    "    accs.append(acc)\n",
    "    train_outs.append(train_out)\n",
    "    valid_outs.append(valid_out)\n",
    "    test_outs.append(test_out)\n",
    "    \n",
    "baccs = np.array(baccs)\n",
    "rmses = np.array(rmses)\n",
    "r2s = np.array(r2s)\n",
    "f1s = np.array(f1s)\n",
    "accs = np.array(accs)\n",
    "\n",
    "\n",
    "with open(os.path.join(ANALYSIS_DIR, \"accuracy_metric.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(baccs, handle)\n",
    "    pickle.dump(rmses, handle)\n",
    "    pickle.dump(r2s, handle)\n",
    "    pickle.dump(f1s, handle)\n",
    "    pickle.dump(accs, handle)\n",
    "\n",
    "with open(os.path.join(ANALYSIS_DIR, \"all_outs.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(train_outs, handle)\n",
    "    pickle.dump(valid_outs, handle)\n",
    "    pickle.dump(test_outs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a9c45",
   "metadata": {},
   "source": [
    "### With 1 Repetition for Sample Test Set Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110af222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: Train 1.16+/-0.00 Valid 1.56+/-0.00 Test 1.50+/-0.00\n",
      "R2:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.99+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.97+/-0.00\n",
      "BACC: Train 1.00+/-0.00 Valid 0.94+/-0.00 Test 0.94+/-0.00\n",
      "RMSE: Train 2.42+/-0.00 Valid 3.21+/-0.00 Test 3.26+/-0.00\n",
      "R2:   Train 0.98+/-0.00 Valid 0.96+/-0.00 Test 0.96+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 0.97+/-0.00 Test 0.97+/-0.00\n",
      "BACC: Train 0.99+/-0.00 Valid 0.94+/-0.00 Test 0.94+/-0.00\n",
      "RMSE: Train 1.06+/-0.00 Valid 1.22+/-0.00 Test 1.91+/-0.00\n",
      "R2:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.99+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 1.00+/-0.00 Test 0.97+/-0.00\n",
      "BACC: Train 0.97+/-0.00 Valid 0.92+/-0.00 Test 0.92+/-0.00\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\",\n",
    "    \"gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 100]_0.01_64\",\n",
    "    \"desc_dnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 0.01]_0.01_64\",\n",
    "]\n",
    "\n",
    "test_outs = []\n",
    "\n",
    "for file in files:\n",
    "    ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "    model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                                   x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                                   l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "    train_out, valid_out, test_out, bacc, rmse, r2, f1, acc = get_metrics(model, ENCODER, IF_REG, IF_CLS,\n",
    "                                                               x_train, y_train, c_train, l_train,\n",
    "                                                               x_valid, y_valid, c_valid, l_valid,\n",
    "                                                               x_test, y_test, c_test, l_test,\n",
    "                                                               n_repeat=1,if_bacc=True)\n",
    "    test_outs.append(test_out)\n",
    "\n",
    "with open(os.path.join(ANALYSIS_DIR, \"test_out.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(test_outs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e29647",
   "metadata": {},
   "source": [
    "# Saliency Map Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48c7b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: Train 1.16+/-0.00 Valid 1.56+/-0.00 Test 1.50+/-0.00\n",
      "R2:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.99+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.97+/-0.00\n",
      "RMSE: Train 1.06+/-0.00 Valid 1.22+/-0.00 Test 1.91+/-0.00\n",
      "R2:   Train 1.00+/-0.00 Valid 0.99+/-0.00 Test 0.99+/-0.00\n",
      "F1:   Train 1.00+/-0.00 Valid 1.00+/-0.00 Test 0.97+/-0.00\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\",\n",
    "    \"desc_dnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 0.01]_0.01_64\",\n",
    "]\n",
    "\n",
    "grads = []\n",
    "\n",
    "for file in files:\n",
    "    ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "    model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                                   x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                                   l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "    train_out, valid_out, test_out, bacc, rmse, r2, f1, acc = get_metrics(model, ENCODER, IF_REG, IF_CLS,\n",
    "                                                               x_train, y_train, c_train, l_train,\n",
    "                                                               x_valid, y_valid, c_valid, l_valid,\n",
    "                                                               x_test, y_test, c_test, l_test,\n",
    "                                                               n_repeat=1, if_bacc=False)\n",
    "\n",
    "    grad = compute_saliency(model, x_train, y_train, l_train, c_train, \n",
    "                             output_index=1, enc_type=ENCODER, if_reg=IF_REG, if_cls=IF_CLS)\n",
    "    \n",
    "    grads.append(grad)\n",
    "\n",
    "with open(os.path.join(ANALYSIS_DIR, \"saliency.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(grads, handle)\n",
    "    pickle.dump(files, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beb168",
   "metadata": {},
   "source": [
    "# Latent Space Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80228cd6",
   "metadata": {},
   "source": [
    "### Different Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84cd4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\",\n",
    "    \"gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 100]_0.01_64\",\n",
    "    \"desc_dnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 0.01]_0.01_64\",\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "    model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                                   x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                                   l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "    latent_train = latent_model(model, data=[x_train, l_train], enc_type=ENCODER, mean_var=False)\n",
    "    latent_valid = latent_model(model, data=[x_valid, l_valid], enc_type=ENCODER, mean_var=False)\n",
    "    latent_test = latent_model(model, data=[x_test, l_test], enc_type=ENCODER, mean_var=False)\n",
    "    \n",
    "    short_name = file.split(\"_20230828\")[0]\n",
    "    \n",
    "    if short_name == \"desc_gnn_cnn\":\n",
    "        short_name = \"\"\n",
    "    elif short_name == \"gnn_cnn\":\n",
    "        short_name = \"_gnn\"\n",
    "    elif short_name == \"desc_dnn_cnn\":\n",
    "        short_name = \"_topo\"\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_DIR, f\"latent_space{short_name}.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(latent_train, handle)\n",
    "        pickle.dump(latent_valid, handle)\n",
    "        pickle.dump(latent_test, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff786dc",
   "metadata": {},
   "source": [
    "### Auxilary Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c231f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"desc_gnn_cnn_20230828_8_val_loss_False_True_1.0_[1.0, 1]_0.001_64\",\n",
    "        \"desc_gnn_cnn_20230828_8_val_loss_True_False_1.0_[1.0, 1]_0.001_64\",\n",
    "        \"desc_gnn_cnn_20230828_8_val_loss_False_False_1.0_[1.0]_0.001_64\",\n",
    "       \n",
    "]\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "    model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                                   x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                                   l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "    latent_train = latent_model(model, data=[x_train, l_train], enc_type=ENCODER, mean_var=False)\n",
    "    latent_valid = latent_model(model, data=[x_valid, l_valid], enc_type=ENCODER, mean_var=False)\n",
    "    latent_test = latent_model(model, data=[x_test, l_test], enc_type=ENCODER, mean_var=False)\n",
    "    \n",
    "    short_name = \"_ref_\" + file.split(\"_\")[-6].lower() + \"_cls_\" + file.split(\"_\")[-5].lower()\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_DIR, f\"latent_space{short_name}.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(latent_train, handle)\n",
    "        pickle.dump(latent_valid, handle)\n",
    "        pickle.dump(latent_test, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919cd26",
   "metadata": {},
   "source": [
    "# UMAP Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b15c5c",
   "metadata": {},
   "source": [
    "### UMAP TopoGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf400b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053af9a5",
   "metadata": {},
   "source": [
    "### UMAP GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c810f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 100]_0.01_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space_gnn.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_gnn.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327f1bc",
   "metadata": {},
   "source": [
    "### UMAP Topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881174b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"desc_dnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 0.01]_0.01_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space_topo.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_topo.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9cc5e",
   "metadata": {},
   "source": [
    "### UMAP No Classsification No Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d8e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"desc_gnn_cnn_20230828_8_val_loss_False_False_1.0_[1.0]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space_reg_false_cls_false.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_reg_false_cls_false.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7b10",
   "metadata": {},
   "source": [
    "### UMAP No Classification Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9bac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"desc_gnn_cnn_20230828_8_val_loss_True_False_1.0_[1.0, 1]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space_reg_true_cls_false.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_reg_true_cls_false.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b69fe",
   "metadata": {},
   "source": [
    "### UMAP No Regression Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85cf842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "file = \"desc_gnn_cnn_20230828_8_val_loss_False_False_1.0_[1.0]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "\n",
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space_reg_false_cls_true.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# UMAP transformation\n",
    "u = umap.UMAP(n_components=2, random_state=0)\n",
    "z = u.fit_transform(latent_all)\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_reg_false_cls_true.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(z, handle)\n",
    "    pickle.dump(u, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa609e",
   "metadata": {},
   "source": [
    "# Polymer Topology Generation Using TopoGNN Latent Space UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0b9af",
   "metadata": {},
   "source": [
    "### TopoGNN Fix Z1, Increase Z2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c22fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(start, end, num_points):\n",
    "    \"\"\"\n",
    "    Interpolate between two points in n-dimensional space.\n",
    "\n",
    "    Parameters:\n",
    "        start (array-like): The starting point as an array-like object.\n",
    "        end (array-like): The ending point as an array-like object.\n",
    "        num_points (int): The number of points to interpolate between start and end.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array containing the interpolated points between start and end.\n",
    "    \"\"\"\n",
    "    start = np.array(start)\n",
    "    end = np.array(end)\n",
    "    t_values = np.linspace(0, 1, num_points)\n",
    "    points = [(1-t)*start + t*end for t in t_values]\n",
    "    return np.array(points)\n",
    "\n",
    "\n",
    "\n",
    "file = \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False) \n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap.pickle\"), \"rb\") as handle:\n",
    "    _ = pickle.load(handle)\n",
    "    u = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a7a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.array([1, 4])\n",
    "end   = np.array([1, 16])\n",
    "num_points = 20\n",
    "max_iter = 50\n",
    "\n",
    "points = interpolate(start, end, num_points)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in range(num_points):\n",
    "    point = points[i]\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        point_ = point + np.random.normal(0, 0.2, size=(2,)) # random pertubation in a neighborhood\n",
    "        data = u.inverse_transform(point_[None, ...])\n",
    "        \n",
    "        G0, G, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s = polymer_generation(model, data, None, ENCODER)\n",
    "        \n",
    "        if check_valid(gen_reg, cln_reg_m, gen_cls, cln_cls):\n",
    "            outputs.append([data, point_, G0, G, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s])\n",
    "            break\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_1_4_1_16.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(outputs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eab4d1",
   "metadata": {},
   "source": [
    "### TopoGNN Fix Z2, Increase Z1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef305232",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.array([-3, 12])\n",
    "end   = np.array([6, 12])\n",
    "num_points = 20\n",
    "max_iter = 50\n",
    "\n",
    "points = interpolate(start, end, num_points)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in range(num_points):\n",
    "    point = points[i]\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        point_ = point + np.random.normal(0, 0.2, size=(2,))\n",
    "        data = u.inverse_transform(point_[None, ...])\n",
    "        \n",
    "        G0, G, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s = polymer_generation(model, data, None, ENCODER)\n",
    "        \n",
    "        if check_valid(gen_reg, cln_reg_m, gen_cls, cln_cls):\n",
    "            outputs.append([data, point_, G0, G, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s])\n",
    "            break\n",
    "            \n",
    "with open(os.path.join(ANALYSIS_DIR, \"umap_-3_12_6_12.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(outputs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e34b56",
   "metadata": {},
   "source": [
    "# Property Guided Topology Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327952db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_isomorphism(graph_list, new_graph):\n",
    "    for graph in graph_list:\n",
    "        if nx.is_isomorphic(graph, new_graph):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def rg_latent_vector(l, y_train, c_train, poly_type='branch', target_rg=40):\n",
    "    \n",
    "    idx = np.where(NAMES == poly_type)[0][0]\n",
    "\n",
    "    a = l[np.where(c_train == idx)[0]]\n",
    "    y = y_train[np.where(c_train == idx)[0]]\n",
    "    \n",
    "    if np.abs(y - target_rg).min() < 1:\n",
    "        \n",
    "        idx2 = np.where(np.abs(y - target_rg) < 1)[0]\n",
    "\n",
    "        return a[idx2], y[idx2]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "085d2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latent space\n",
    "with open(os.path.join(ANALYSIS_DIR, \"latent_space.pickle\"), \"rb\") as handle:\n",
    "    latent_train = pickle.load(handle)\n",
    "    latent_valid = pickle.load(handle)\n",
    "    latent_test = pickle.load(handle)\n",
    "    \n",
    "latent_all = np.concatenate((latent_train, latent_valid, latent_test), axis=0)\n",
    "\n",
    "# Load data label\n",
    "((x_train, y_train, c_train, l_train, graph_train),\n",
    "(x_valid, y_valid, c_valid, l_valid, graph_valid),\n",
    "(x_test, y_test, c_test, l_test, graph_test),\n",
    "NAMES, SCALER, LE) = load_data(os.path.join(DATA_DIR, 'rg2.pickle'), fold=0, if_validation=True)\n",
    "\n",
    "graph_all = np.concatenate((graph_train, graph_valid, graph_test))\n",
    "y_all = np.concatenate((y_train, y_valid, y_test))\n",
    "c_all = np.concatenate((c_train, c_valid, c_test))\n",
    "\n",
    "# Load TopoGNN model\n",
    "file = \"desc_gnn_cnn_20230828_8_val_decoder_acc_True_True_1.0_[1.0, 1, 1]_0.001_64\"\n",
    "\n",
    "ENCODER, DECODER, MONITOR, IF_REG, IF_CLS, weights, LR, BS = get_spec(file)\n",
    "\n",
    "model, pickle_file = train_vae(ENCODER, DECODER, MONITOR, IF_REG, IF_CLS,\n",
    "                               x_train, x_valid, y_train, y_valid, c_train, c_valid,\n",
    "                               l_train, l_valid, 1.0, weights, LR, BS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "810ae1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prop_polymer(target_rg=30, target_top=\"branch\", max_iter=1000):\n",
    "    outputs = []\n",
    "    graphs = []\n",
    "\n",
    "    latent_vector, rg_dataset = rg_latent_vector(latent_all, y_all, c_all, target_top, target_rg)\n",
    "    \n",
    "    if latent_vector is None:\n",
    "        raise Exception(\"The target rg2 is too large/small \")\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        noise = np.random.normal(0, 1, (1, 8)) * 0.1\n",
    "        num = len(latent_vector)\n",
    "\n",
    "        for j in range(num):\n",
    "            K.clear_session()\n",
    "            d_in = latent_vector[j, ...] + noise\n",
    "            graph_raw, graph, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s = polymer_generation(model, d_in, None, ENCODER)\n",
    "\n",
    "            flag1 = np.abs(gen_reg - cln_reg_m) < 2\n",
    "            flag2 = np.abs(gen_reg - target_rg) < 2\n",
    "            flag3 = np.abs(cln_reg_m - target_rg) < 2\n",
    "            flag4 = gen_cls == cln_cls\n",
    "            flag5 = cln_cls == target_top\n",
    "\n",
    "            x_clean_ = nx.to_numpy_array(graph)\n",
    "            n_clean = len(x_clean_)\n",
    "            x_clean = np.zeros((1, 100, 100))\n",
    "            x_clean[0, :n_clean, :n_clean] = x_clean_\n",
    "            x_clean = x_clean.astype(\"int\")\n",
    "\n",
    "            l_clean = get_desc(graph)[None, ...]\n",
    "            l_clean = SCALER.transform(l_clean)\n",
    "\n",
    "            d_clean = latent_model(model, data=[x_clean, l_clean], enc_type=ENCODER, mean_var=False)\n",
    "\n",
    "            if flag1 and flag2 and flag3 and flag4 and flag5: \n",
    "                if len(graphs) > 0:\n",
    "                    if not check_isomorphism(graphs, graph) and not check_isomorphism(graph_all, graph):\n",
    "                        graphs.append(graph)\n",
    "                        outputs.append([d_in, graph_raw, graph, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s, latent_vector[j, ...], d_in, d_clean])\n",
    "                else:\n",
    "                    graphs.append(graph)\n",
    "                    outputs.append([d_in, graph_raw, graph, gen_cls, cln_cls, gen_reg, cln_reg_m, cln_reg_s, latent_vector[j, ...], d_in, d_clean])\n",
    "\n",
    "    \n",
    "    # check latent space distance\n",
    "    z_cleans = []\n",
    "    z_raws   = []\n",
    "    rmses    = []\n",
    "    new_outputs = []\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        graph = outputs[i][2]\n",
    "        x_clean_ = nx.to_numpy_array(graph)\n",
    "        n_clean = len(x_clean_)\n",
    "        x_clean = np.zeros((1, 100, 100))\n",
    "        x_clean[0, :n_clean, :n_clean] = x_clean_\n",
    "        x_clean = x_clean.astype(\"int\")\n",
    "\n",
    "        l_clean = get_desc(graph)[None, ...]\n",
    "        l_clean = SCALER.transform(l_clean)\n",
    "\n",
    "        z_clean = latent_model(model, data=[x_clean, l_clean], enc_type=ENCODER, mean_var=False).squeeze()\n",
    "\n",
    "        z_raw = outputs[i][0].squeeze()\n",
    "        rmse = skm.mean_absolute_error(z_raw, z_clean)\n",
    "\n",
    "        if rmse < 1:\n",
    "            z_cleans.append(z_clean)\n",
    "            z_raws.append(z_raw)\n",
    "            rmses.append(rmse)\n",
    "            new_outputs.append(outputs[i]+[z_clean])\n",
    "            \n",
    "    return new_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b79e6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rg = 30\n",
    "\n",
    "for target_top in [\"star\", \"branch\", \"comb\", \"cyclic\"]:\n",
    "    new_outputs = gen_prop_polymer(target_rg=target_rg, target_top=target_top, max_iter=1000)\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_DIR, f\"gen_{target_top}_{target_rg}_all.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(new_outputs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4c605dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rg = 50\n",
    "\n",
    "for target_top in [\"branch\", \"comb\"]:\n",
    "    new_outputs = gen_prop_polymer(target_rg=target_rg, target_top=target_top, max_iter=1000)\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_DIR, f\"gen_{target_top}_{target_rg}_all.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(new_outputs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11effa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rg = 7.5\n",
    "\n",
    "for target_top in [\"star\", \"dendrimer\"]:\n",
    "    new_outputs = gen_prop_polymer(target_rg=target_rg, target_top=target_top, max_iter=1000)\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_DIR, f\"gen_{target_top}_{target_rg}_all.pickle\"), \"wb\") as handle:\n",
    "        pickle.dump(new_outputs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2a819",
   "metadata": {},
   "source": [
    "# Diversity Calculation Using Vendi Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f437e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all graphs into graph eigen spectra\n",
    "graph_total = [graph_train, graph_valid, graph_test]\n",
    "\n",
    "lap_spec_data = []\n",
    "\n",
    "for graphs in graph_total:\n",
    "    for G in graphs:\n",
    "        lap_spec = nx.laplacian_spectrum(G)\n",
    "        lap_spec_zero_pad = np.zeros((100,))\n",
    "        lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "        lap_spec_data.append(lap_spec_zero_pad)\n",
    "        \n",
    "lap_spec_data = np.array(lap_spec_data)\n",
    "\n",
    "with open(os.path.join(ANALYSIS_DIR, \"lap_spec_data.pickle\"), \"wb\") as handle:\n",
    "    pickle.dump(lap_spec_data, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634b4c3",
   "metadata": {},
   "source": [
    "### Dataset Vendi Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95d0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 2.0968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81899ae6",
   "metadata": {},
   "source": [
    "### Vendi Score for the Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5bf305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_space_desc_gnn_cnn\n",
      "Dataset Vendi Score: 7.3225 \n",
      "\n",
      "latent_space_gnn_cnn\n",
      "Dataset Vendi Score: 7.4370 \n",
      "\n",
      "latent_space_desc_dnn_cnn\n",
      "Dataset Vendi Score: 7.0863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"latent_space_desc_gnn_cnn.pickle\",\n",
    "    \"latent_space_gnn_cnn.pickle\",\n",
    "    \"latent_space_desc_dnn_cnn.pickle\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(ANALYSIS_DIR, file), \"rb\") as handle:\n",
    "        latent_data = pickle.load(handle)\n",
    "    print(file.split(\".pickle\")[0])\n",
    "    print(f\"Dataset Vendi Score: {vendi.score_dual(latent_data):0.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccb5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_space_False_False\n",
      "Dataset Vendi Score: 5.8532 \n",
      "\n",
      "latent_space_False_True\n",
      "Dataset Vendi Score: 6.3171 \n",
      "\n",
      "latent_space_True_False\n",
      "Dataset Vendi Score: 5.3128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"latent_space_False_False.pickle\",\n",
    "    \"latent_space_False_True.pickle\",\n",
    "    \"latent_space_True_False.pickle\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(ANALYSIS_DIR, file), \"rb\") as handle:\n",
    "        latent_data = pickle.load(handle)\n",
    "        print(file.split(\".pickle\")[0])\n",
    "    print(f\"Dataset Vendi Score: {vendi.score_dual(latent_data):0.4f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f9469",
   "metadata": {},
   "source": [
    "### Vendi Score for Random Generation Based on Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef0825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 5.0684\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ANALYSIS_DIR, \"no_valid_random_gen_desc_gnn_cnn.pickle\"), \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = []\n",
    "\n",
    "for G in gen_clean_graph:\n",
    "    lap_spec = nx.laplacian_spectrum(G)\n",
    "    lap_spec_zero_pad = np.zeros((100,))\n",
    "    lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "    lap_spec_data.append(lap_spec_zero_pad)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2972ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 4.9580\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ANALYSIS_DIR, \"no_valid_random_gen_gnn_cnn.pickle\"), \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = []\n",
    "\n",
    "for G in gen_clean_graph:\n",
    "    lap_spec = nx.laplacian_spectrum(G)\n",
    "    lap_spec_zero_pad = np.zeros((100,))\n",
    "    lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "    lap_spec_data.append(lap_spec_zero_pad)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e6a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vendi Score: 4.3305\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ANALYSIS_DIR, \"no_valid_random_gen_desc_dnn_cnn.pickle\"), \"rb\") as handle:\n",
    "    gen_data = pickle.load(handle)\n",
    "    \n",
    "gen_clean_graph = [gen_data[i][2] for i in range(len(gen_data))]\n",
    "\n",
    "lap_spec_data = []\n",
    "\n",
    "for G in gen_clean_graph:\n",
    "    lap_spec = nx.laplacian_spectrum(G)\n",
    "    lap_spec_zero_pad = np.zeros((100,))\n",
    "    lap_spec_zero_pad[:len(lap_spec)] = lap_spec\n",
    "    lap_spec_data.append(lap_spec_zero_pad)\n",
    "\n",
    "print(f\"Dataset Vendi Score: {vendi.score_dual(lap_spec_data):0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38torch113 [~/.conda/envs/py38torch113/]",
   "language": "python",
   "name": "conda_py38torch113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
